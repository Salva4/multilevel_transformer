#!/bin/bash -l
#SBATCH --job-name="job02"
#SBATCH --account="c24"
#SBATCH --mail-type=ALL
#SBATCH --time=24:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-core=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --partition=normal
#SBATCH --constraint=gpu
#SBATCH --hint=nomultithread
#SBATCH --output=/users/msalvado/prova_transfpytorch/multilevel_transformer/task1__transformer_revisit/outputs/continuous_transformer__20231002_revisit_conventional_training/outputs/ContTrans_nnodes1_ntasksxnode1_procgpu_lr1e-2_optimAdam_initNone_petorch_N4_T04_epochs1000000000_batchsize8.txt
#SBATCH --error=/users/msalvado/prova_transfpytorch/multilevel_transformer/task1__transformer_revisit/outputs/continuous_transformer__20231002_revisit_conventional_training/errors/error_flags_ContTrans_nnodes1_ntasksxnode1_procgpu_lr1e-2_optimAdam_initNone_petorch_N4_T04_epochs1000000000_batchsize8.txt

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export PYTHONUNBUFFERED=true

RUNPATH=/users/msalvado/prova_transfpytorch/multilevel_transformer/task1__transformer_revisit/src
cd $RUNPATH
conda activate env1

srun python3 -u main.py --output_fn ContTrans_nnodes1_ntasksxnode1_procgpu_lr1e-2_optimAdam_initNone_petorch_N4_T01_epochs1000000000_batchsize8 --models_dir /users/msalvado/MLT/ML_PQ/outputs/continuous_transformer__20221203_01_samebut_Adam/models --model continuoustransformer --lr 1e-2 --optim Adam --init None --pe torch --N 4 --T 4.0 --num_epochs 1000000000 --batch_size 8
