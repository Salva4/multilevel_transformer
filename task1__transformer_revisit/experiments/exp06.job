#!/bin/bash -l
#SBATCH --job-name="job06_gpu_alt2"
#SBATCH --account="c24"
#SBATCH --mail-type=ALL
#SBATCH --time=10:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-core=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --partition=normal
#SBATCH --constraint=gpu
#SBATCH --hint=nomultithread
#SBATCH --error=/users/msalvado/MLT/ML_PQ/outputs/continuous_transformer__20221203_01_samebut_Adam/errors/error_flags_ContTrans_nnodes1_ntasksxnode1_procgpu_lr1e-2_optimAdam_initxavier_petorch_N4_T02_epochs1000000000_batchsize16_nlvls2_nVcycles1_musnus1--1-1_lrMGOPT-1.00.txt
#SBATCH --output=/users/msalvado/MLT/ML_PQ/outputs/continuous_transformer__20221203_01_samebut_Adam/outputs/ContTrans_nnodes1_ntasksxnode1_procgpu_lr1e-2_optimAdam_initxavier_petorch_N4_T02_epochs1000000000_batchsize16_nlvls2_nVcycles1_musnus1--1-1_lrMGOPT-1.00.txt

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export PYTHONUNBUFFERED=true

RUNPATH=/users/msalvado/MLT/ML_PQ/src
cd $RUNPATH
source activate MLT_gpu

srun python3 -u main.py --output_fn ContTrans_nnodes1_ntasksxnode1_procgpu_lr1e-2_optimAdam_initxavier_petorch_N4_T02_epochs1000000000_batchsize16_nlvls2_nVcycles1_musnus1--1-1_lrMGOPT-1.00 --models_dir /users/msalvado/MLT/ML_PQ/outputs/continuous_transformer__20221203_01_samebut_Adam/models --model continuoustransformer --lr 1e-2 --optim Adam --init xavier --pe torch --N 4 --T 2.0 --num_epochs 1000000000 --batch_size 16 --n_monitoring 1 --n_lvls 2 --n_V_cycles 1 --mus_nus 1_1-1 --lr_MGOPT -1.0